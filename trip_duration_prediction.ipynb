{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb611a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries importation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import holidays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d2375",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data insertion and initial processing\n",
    "csv_path = \"./nyc_taxi_data_2014.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Use a random sample for a faster development and testing cycle.\n",
    "# Comment out this line to use the full dataset.\n",
    "df = df.sample(n=200_000, random_state=42)\n",
    "print(f\"number of records: {len(df)}\")\n",
    "\n",
    "# Feature Selection & Creation\n",
    "cols_to_use = [\n",
    "    'pickup_datetime', 'dropoff_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'vendor_id',\n",
    "    'store_and_fwd_flag', 'tolls_amount'\n",
    "]\n",
    "df = df[cols_to_use].copy()\n",
    "\n",
    "# Time processing\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], errors='coerce')\n",
    "df.dropna(subset=['pickup_datetime', 'dropoff_datetime'], inplace=True)\n",
    "\n",
    "# Generated temporal features\n",
    "df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "df['day_of_week'] = df['pickup_datetime'].dt.weekday\n",
    "df['month'] = df['pickup_datetime'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Trip duration\n",
    "df['trip_duration_min'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "df = df[df['trip_duration_min'] > 0].copy()\n",
    "\n",
    "# Function to calculate haversine distance on Earth's surface\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  # km\n",
    "    if pd.isna(lon1) or pd.isna(lat1) or pd.isna(lon2) or pd.isna(lat2):\n",
    "        return np.nan\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "df['trip_distance_km'] = df.apply(\n",
    "    lambda r: haversine(r['pickup_longitude'], r['pickup_latitude'],\n",
    "                        r['dropoff_longitude'], r['dropoff_latitude']), axis=1)\n",
    "\n",
    "# Average speed\n",
    "df['avg_speed_kmph'] = df['trip_distance_km'] / (df['trip_duration_min'] / 60)\n",
    "\n",
    "df = df.dropna(subset=['trip_distance_km', 'avg_speed_kmph'])\n",
    "\n",
    "# Clustering pickup and dropoff locations\n",
    "coords_pickup = df[['pickup_latitude', 'pickup_longitude']].dropna()\n",
    "kmeans_pickup = KMeans(n_clusters=50, random_state=42, n_init=10)\n",
    "pickup_clusters = kmeans_pickup.fit_predict(coords_pickup)\n",
    "df.loc[coords_pickup.index, 'pickup_cluster'] = pickup_clusters\n",
    "\n",
    "coords_dropoff = df[['dropoff_latitude', 'dropoff_longitude']].dropna()\n",
    "kmeans_dropoff = KMeans(n_clusters=50, random_state=42, n_init=10)\n",
    "dropoff_clusters = kmeans_dropoff.fit_predict(coords_dropoff)\n",
    "df.loc[coords_dropoff.index, 'dropoff_cluster'] = dropoff_clusters\n",
    "\n",
    "# Holiday detection\n",
    "try:\n",
    "    unique_years = df['pickup_datetime'].dt.year.unique()\n",
    "    us_holidays = holidays.UnitedStates(years=unique_years)\n",
    "    df['is_holiday'] = df['pickup_datetime'].dt.date.apply(lambda d: d in us_holidays).astype(int)\n",
    "except Exception as e:\n",
    "    print(f\"Error detecting holidays: {e}\")\n",
    "    df['is_holiday'] = 0\n",
    "\n",
    "# Bearing angle\n",
    "def bearing(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = sin(dlon) * cos(lat2)\n",
    "    y = cos(lat1)*sin(lat2) - sin(lat1)*cos(lat2)*cos(dlon)\n",
    "    theta = atan2(x, y)\n",
    "    bearing_deg = (np.degrees(theta) + 360) % 360\n",
    "    return bearing_deg\n",
    "\n",
    "df['bearing'] = df.apply(\n",
    "    lambda r: bearing(r['pickup_latitude'], r['pickup_longitude'],\n",
    "                      r['dropoff_latitude'], r['dropoff_longitude']), axis=1)\n",
    "\n",
    "# Weather data\n",
    "df_weather = pd.read_csv(\"weather_nyc_2014.csv\")\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df['date'] = df['pickup_datetime'].dt.date\n",
    "df_weather['date'] = df_weather['date'].dt.date\n",
    "\n",
    "# Merge weather data with taxi data based on pickup date\n",
    "df = df.merge(df_weather, on='date', how='left')\n",
    "df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# Composite weather feature\n",
    "df['weather_impact'] = df['precip_mm'] * 2 + df['windgust_kph'] * 0.1 - df['avg_temp_c'] * 0.05\n",
    "\n",
    "# Temporal-spatial interaction features\n",
    "df['is_peak_hour'] = df['pickup_hour'].apply(lambda h: 1 if (7 <= h <= 9 or 16 <= h <= 19) else 0)\n",
    "df['holiday_peak_flag'] = ((df['is_holiday'] == 1) & (df['is_peak_hour'] == 1)).astype(int)\n",
    "\n",
    "# Trip count feature per cluster and hour\n",
    "df['pickup_cluster_hour'] = df['pickup_cluster'].astype(str) + '_' + df['pickup_hour'].astype(str)\n",
    "cluster_hour_counts = df.groupby('pickup_cluster_hour').size()\n",
    "df['pickup_cluster_hour_trip_count'] = df['pickup_cluster_hour'].map(cluster_hour_counts).fillna(0)\n",
    "df.drop(columns=['pickup_cluster_hour'], inplace=True)\n",
    "\n",
    "df.fillna({'store_and_fwd_flag': 'U'}, inplace=True)\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Final number of records: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98867171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull().sum()\n",
    "\n",
    "# df.columns\n",
    "\n",
    "# df.hist(figsize=(20, 15))  \n",
    "# plt.tight_layout()         \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7f453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Selection and Data Splitting ---\n",
    "\n",
    "# Define the target and feature columns for the model\n",
    "target = 'trip_duration_min'\n",
    "features = [\n",
    "    'trip_distance_km',\n",
    "    'vendor_id',\n",
    "    'pickup_cluster',\n",
    "    'dropoff_cluster',\n",
    "    'pickup_cluster_hour_trip_count',\n",
    "    'passenger_count',\n",
    "    \n",
    "    # Spatial Features\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'bearing',\n",
    "    \n",
    "    # Temporal Features\n",
    "    'pickup_hour',\n",
    "    'day_of_week',\n",
    "    # 'month',  # Excluded based on correlation analysis\n",
    "    'is_weekend',\n",
    "    'is_peak_hour',\n",
    "    'holiday_peak_flag',\n",
    "    \n",
    "    # Weather Features\n",
    "    'weather_impact',\n",
    "    'avg_temp_c',\n",
    "    # 'precip_mm',  # Excluded based on low feature importance\n",
    "    'windgust_kph',\n",
    "    'condition',\n",
    "]\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df[features]\n",
    "Y = df[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# --- Data Encoding ---\n",
    "\n",
    "# One-Hot Encoding\n",
    "# This approach is good for tree-based models like XGBoost as it doesn't assume any ordinal relationship.\n",
    "X_train_ohe = pd.get_dummies(X_train)\n",
    "X_test_ohe = pd.get_dummies(X_test)\n",
    "\n",
    "# Align columns to ensure the same features are in both train and test sets\n",
    "X_train_ohe, X_test_ohe = X_train_ohe.align(X_test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "\n",
    "# Label Encoding\n",
    "# This is an alternative, more memory-efficient approach for tree-based models.\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "encoders = {}\n",
    "\n",
    "X_train_le = X_train.copy()\n",
    "X_test_le = X_test.copy()\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_le[col] = le.fit_transform(X_train[col])\n",
    "    X_test_le[col] = le.transform(X_test[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"Shape of One-Hot Encoded data:\", X_train_ohe.shape)\n",
    "print(\"Shape of Label Encoded data:\", X_train_le.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1719c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c781901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Regressor Model Training and Evaluation (Basic) ---\n",
    "\n",
    "# Set the training and testing data\n",
    "X_train = X_train_ohe\n",
    "X_test = X_test_ohe\n",
    "\n",
    "# Initialize and train the XGBoost model with default hyperparameters\n",
    "model = XGBRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Reporting the model's performance ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost Model with Cross-Validation ---\n",
    "\n",
    "# Set the training and testing data\n",
    "# We use the One-Hot Encoded data as it performs well with XGBoost\n",
    "X_train = X_train_ohe\n",
    "X_test = X_test_ohe\n",
    "\n",
    "# Initialize the XGBoost model with a baseline number of estimators\n",
    "model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the scoring metrics for cross-validation\n",
    "# Note: Negative scores are used for metrics that should be minimized (MAE, MSE).\n",
    "# cross_validate will automatically handle this by maximizing the score.\n",
    "scoring = {\n",
    "    'MAE': 'neg_mean_absolute_error',\n",
    "    'MSE': 'neg_mean_squared_error',\n",
    "    'R2': 'r2'\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cv_results = cross_validate(\n",
    "    model,\n",
    "    X_train_ohe, \n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1  # Use all available CPU cores for faster processing\n",
    ")\n",
    "\n",
    "# You can then print and analyze the results\n",
    "# print(\"Cross-Validation Results:\")\n",
    "# for score_name, scores in cv_results.items():\n",
    "#     print(f\"{score_name}: {scores.mean():.4f} +/- {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f9fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd57f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyzing Cross-Validation Results ---\n",
    "\n",
    "# Loop through each metric to print the results\n",
    "for metric in scoring.keys():\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    train_scores = cv_results[f'train_{metric}']\n",
    "    \n",
    "    # Invert the score for metrics that were minimized (like MAE and MSE)\n",
    "    if 'neg' in scoring[metric]:\n",
    "        test_scores = -test_scores\n",
    "        train_scores = -train_scores\n",
    "\n",
    "    # Print the mean and standard deviation of the scores\n",
    "    print(f\"Train {metric}: {np.mean(train_scores):.3f} ± {np.std(train_scores):.3f}\")\n",
    "    print(f\"Test  {metric}: {np.mean(test_scores):.3f} ± {np.std(test_scores):.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998dcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final XGBoost Model Training and Evaluation ---\n",
    "\n",
    "# Set the training and testing data\n",
    "X_train = X_train_ohe\n",
    "X_test = X_test_ohe\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the final model with the best parameters\n",
    "model = XGBRegressor(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=250,\n",
    "    max_depth=10,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model on the full training set\n",
    "model.fit(X_train, y_train)pip install numpy==2.3.2\n",
    "\n",
    "\n",
    "# Make predictions on the unseen test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Reporting the Final Performance ---\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5 \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859b20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1ad23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ad6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f97937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyzing Feature Importance from the Model ---\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Get the names of the features\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame to hold feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=True)\n",
    "\n",
    "# Plot the feature importance scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance from XGBoost Model\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n",
    "# Print the feature importance table\n",
    "print(\"--- Feature Importance Table ---\")\n",
    "print(feature_importance_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9da10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Permutation Importance Analysis ---\n",
    "\n",
    "# Calculate permutation importance on the test data\n",
    "# This method measures how much the model's performance decreases\n",
    "# when a single feature's values are randomly shuffled.\n",
    "result = permutation_importance(\n",
    "    model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores for faster processing\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "\n",
    "# Sort the features by their importance score\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X_test.columns[perm_sorted_idx],\n",
    "    'Importance': result.importances_mean[perm_sorted_idx],\n",
    "    'Std': result.importances_std[perm_sorted_idx]\n",
    "})\n",
    "\n",
    "print(\"\\nPermutation Importance Results:\")\n",
    "print(perm_importance_df.sort_values(by='Importance', ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adf8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analyzing Model Errors ---\n",
    "\n",
    "# Create a copy of the test data for error analysis\n",
    "error_df = X_test.copy() \n",
    "error_df['actual_duration'] = y_test\n",
    "error_df['predicted_duration'] = y_pred\n",
    "\n",
    "# Calculate the error and absolute error\n",
    "error_df['error'] = error_df['predicted_duration'] - error_df['actual_duration']\n",
    "error_df['abs_error'] = np.abs(error_df['error'])\n",
    "\n",
    "# Filter trips with error above a certain threshold (e.g., 20 minutes)\n",
    "error_threshold = 20\n",
    "high_error_trips = error_df[error_df['abs_error'] > error_threshold].copy()\n",
    "\n",
    "# Sort by the highest error to see the worst predictions\n",
    "high_error_trips = high_error_trips.sort_values(by='abs_error', ascending=False)\n",
    "\n",
    "print(f\"Number of trips with error greater than {error_threshold} minutes: {len(high_error_trips)} trips\")\n",
    "\n",
    "print(\"\\nTrips with the highest error:\")\n",
    "print(high_error_trips.head())\n",
    "\n",
    "print(\"\\nAnalysis of high-error trips by pickup hour:\")\n",
    "print(high_error_trips['pickup_hour'].value_counts(normalize=True).head())\n",
    "\n",
    "print(\"\\nAnalysis of high-error trips by pickup cluster:\")\n",
    "print(high_error_trips['pickup_cluster'].value_counts(normalize=True).head())\n",
    "\n",
    "print(\"\\nAnalysis of high-error trips by dropoff cluster:\")\n",
    "print(high_error_trips['dropoff_cluster'].value_counts(normalize=True).head())\n",
    "\n",
    "print(\"\\nDescription of trip distance for high-error trips:\")\n",
    "print(high_error_trips['trip_distance_km'].describe())\n",
    "\n",
    "print(f\"Under-predicted duration (model was optimistic): {100 * (high_error_trips['error'] < 0).mean():.2f}%\")\n",
    "print(f\"Over-predicted duration (model was pessimistic): {100 * (high_error_trips['error'] > 0).mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed122413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical Analysis of Prediction Errors ---\n",
    "\n",
    "# Calculate errors and absolute errors\n",
    "errors = y_test - y_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "# Create a DataFrame to hold all error-related information\n",
    "df_error = X_test.copy()\n",
    "df_error['y_true'] = y_test\n",
    "df_error['y_pred'] = y_pred\n",
    "df_error['error'] = errors\n",
    "df_error['abs_error'] = abs_errors\n",
    "\n",
    "# Print key statistics of the errors\n",
    "print(\"\\nError statistics:\")\n",
    "print(df_error[['error', 'abs_error']].describe())\n",
    "\n",
    "# 1. Visualize the error distribution\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.histplot(df_error['error'], bins=50, kde=True, color='coral')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.xlabel('Error (Predicted - Actual)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eca172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c2fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a2b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correlation Analysis and Visualization\n",
    "\n",
    "# # The target variable for our analysis\n",
    "# target_col = 'trip_duration_min'\n",
    "\n",
    "# # Calculate the correlation matrix for all numeric features\n",
    "# corr_matrix = df.corr(numeric_only=True)\n",
    "\n",
    "# # Heatmap of feature correlations\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # Correlation of each feature with the target variable\n",
    "# target_corr = corr_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.barplot(x=target_corr.values, y=target_corr.index, palette='viridis')\n",
    "# plt.title(f'Correlation with Target: {target_col}')\n",
    "# plt.xlabel('Correlation')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # print(\"Correlation with target:\")\n",
    "# # print(target_corr)\n",
    "\n",
    "# # print(\"\\nFull correlation matrix:\")\n",
    "# # print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Analysis: Correlation & Mutual Information\n",
    "\n",
    "# target_col = 'trip_duration_min'\n",
    "# X = df.drop(columns=[target_col])\n",
    "# y = df[target_col]\n",
    "\n",
    "# # --- Pearson and Spearman Correlation Comparison ---\n",
    "# num_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# # Ensure the target column is included for correlation calculation\n",
    "# if target_col not in num_df.columns:\n",
    "#     num_df[target_col] = df[target_col]\n",
    "\n",
    "# if target_col in num_df.columns:\n",
    "#     pearson_corr = num_df.corr(method='pearson')[target_col].drop(target_col)\n",
    "#     spearman_corr = num_df.corr(method='spearman')[target_col].drop(target_col)\n",
    "\n",
    "#     comparison = pd.DataFrame({\n",
    "#         'Pearson': pearson_corr,\n",
    "#         'Spearman': spearman_corr,\n",
    "#         'Diff': (pearson_corr - spearman_corr).abs()\n",
    "#     }).sort_values('Diff', ascending=False)\n",
    "\n",
    "#     print(\"--- Correlation Comparison with Target ---\\n\")\n",
    "#     print(comparison.to_string())\n",
    "#     print(\"\\n-------------------------------------------\")\n",
    "\n",
    "# # --- Mutual Information ---\n",
    "# datetime_cols = X.select_dtypes(include=['datetime64[ns]', 'datetime']).columns\n",
    "\n",
    "# cols_to_keep = X.columns.difference(datetime_cols)\n",
    "# X_subset_for_mi = X[cols_to_keep]\n",
    "# X_encoded = pd.get_dummies(X_subset_for_mi, drop_first=True)\n",
    "\n",
    "# mi_scores = mutual_info_regression(X_encoded, y, random_state=0)\n",
    "# mi_series = pd.Series(mi_scores, index=X_encoded.columns).sort_values(ascending=False)\n",
    "\n",
    "# # Mutual Information Feature Plot\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# mi_series.head(20).plot(kind='barh', color='teal')\n",
    "# plt.title('Top 20 MI Features with Target')\n",
    "# plt.xlabel('Mutual Information')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416438a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- K-Nearest Neighbors (KNN) Model Training and Evaluation ---\n",
    "\n",
    "# # Set the training and testing data\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Scaling the data is crucial for distance-based models like KNN\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize and train the KNN model\n",
    "# # Using n_neighbors=15 as an example hyperparameter\n",
    "# knn_model = KNeighborsRegressor(n_neighbors=15)\n",
    "# knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make predictions on the scaled test data\n",
    "# y_pred = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# # --- Reporting the model's performance ---\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26894305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Linear Regression Model Training and Evaluation ---\n",
    "\n",
    "# # Set the training and testing data\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Scaling the data is essential for linear models\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize and train the Linear Regression model\n",
    "# lr_model = LinearRegression()\n",
    "# lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Make predictions on the scaled test data\n",
    "# y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# # --- Reporting the model's performance ---\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SVR\n",
    "\n",
    "\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# #scaling\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# svr_model = SVR()\n",
    "# svr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# y_pred = svr_model.predict(X_test_scaled)\n",
    "\n",
    "# #Report\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# rmse = mse ** 0.5 \n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Decision Tree Regressor Model Training and Evaluation ---\n",
    "\n",
    "# # Set the training and testing data\n",
    "# # Decision Tree models are not sensitive to feature scaling,\n",
    "# # so we can use the raw encoded data.\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Initialize and train the Decision Tree model\n",
    "# dt_model = DecisionTreeRegressor(random_state=42)\n",
    "# dt_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test data\n",
    "# y_pred = dt_model.predict(X_test)\n",
    "\n",
    "\n",
    "# # --- Reporting the model's performance ---\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa429c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Random Forest Regressor Model Training and Evaluation ---\n",
    "\n",
    "# # Set the training and testing data\n",
    "# # Random Forest, like other tree-based models, does not require feature scaling.\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Initialize and train the Random Forest model\n",
    "# # Using n_estimators=100 as a starting point.\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # --- Reporting the model's performance ---\n",
    "# # Make predictions on the test data\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Random Forest Regressor Model (with Scaled Data) ---\n",
    "\n",
    "# # Set the training and testing data\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Scale the data to test its effect on the model\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize and train the Random Forest model\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # --- Reporting the model's performance ---\n",
    "# # Make predictions on the scaled test data\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"MAE: {mae:.2f}\")\n",
    "# print(f\"MSE: {mse:.2f}\")\n",
    "# print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddc79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "\n",
    "# # Use the One-Hot Encoded data for XGBoost\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# # Initialize XGBoost model\n",
    "# xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# # Define the hyperparameters to tune\n",
    "# param_distributions = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [3, 5, 7, 10],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.7, 0.8, 1.0],\n",
    "#     'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "# }\n",
    "\n",
    "# # Define the evaluation metrics\n",
    "# scoring = {\n",
    "#     'R2': 'r2',\n",
    "#     'MAE': 'neg_mean_absolute_error',\n",
    "#     'MSE': 'neg_mean_squared_error'\n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_distributions=param_distributions,\n",
    "#     n_iter=50,\n",
    "#     cv=5,\n",
    "#     scoring=scoring,\n",
    "#     refit='MAE',\n",
    "#     n_jobs=-1,  # Use all available cores for speed\n",
    "#     random_state=42,\n",
    "#     return_train_score=True\n",
    "# )\n",
    "\n",
    "# # Run the search on the training data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # --- Analyze the Results ---\n",
    "\n",
    "# # Find and print the best parameters\n",
    "# print(\"\\nBest Parameters:\")\n",
    "# print(random_search.best_params_)\n",
    "\n",
    "# # Analyze results and convert negative scores to positive\n",
    "# results_df = pd.DataFrame(random_search.cv_results_)\n",
    "# results_df['mean_test_MAE'] = -results_df['mean_test_MAE']\n",
    "# results_df['mean_test_MSE'] = -results_df['mean_test_MSE']\n",
    "# results_df = results_df.sort_values(by='rank_test_MAE')\n",
    "\n",
    "# print(\"\\nTop 5 parameter sets by MAE:\")\n",
    "# print(results_df[[ 'params', 'mean_test_MAE', 'mean_test_MSE', 'mean_test_R2' ]].head(5))\n",
    "\n",
    "# # --- Final Evaluation on Test Set ---\n",
    "\n",
    "# # Get the best model from the search\n",
    "# best_model = random_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# final_mae = mean_absolute_error(y_test, y_pred)\n",
    "# final_mse = mean_squared_error(y_test, y_pred)\n",
    "# final_r2 = r2_score(y_test, y_pred)\n",
    "# final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "# print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "# print(f\"MAE: {final_mae:.2f}\")\n",
    "# print(f\"MSE: {final_mse:.2f}\")\n",
    "# print(f\"R² : {final_r2:.2f}\")\n",
    "# print(f\"RMSE: {final_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Grid Search based on best parameters from Randomized Search ---\n",
    "\n",
    "# X_train = X_train_ohe\n",
    "# X_test = X_test_ohe\n",
    "\n",
    "# xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# # Define the hyperparameter grid for fine-tuning\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 250],\n",
    "#     'max_depth': [8, 10, 12],\n",
    "#     'learning_rate': [0.05, 0.1, 0.15],\n",
    "#     'subsample': [0.7, 0.8, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "# }\n",
    "\n",
    "# # Define the evaluation metrics\n",
    "# scoring = {\n",
    "#     'R2': 'r2',\n",
    "#     'MAE': 'neg_mean_absolute_error',\n",
    "#     'MSE': 'neg_mean_squared_error'\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring=scoring,\n",
    "#     refit='MAE',  # Refit the best model based on the MAE score\n",
    "#     cv=5,\n",
    "#     n_jobs=-1,\n",
    "#     return_train_score=True\n",
    "# )\n",
    "\n",
    "# # Run the grid search\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # --- Analyze the Results ---\n",
    "\n",
    "# # Print the best parameters based on MAE\n",
    "# print(\"\\nBest Parameters based on MAE:\")\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "# # Get the best model from the search\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Make predictions on the final test set\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Calculate final metrics\n",
    "# final_mae = mean_absolute_error(y_test, y_pred)\n",
    "# final_mse = mean_squared_error(y_test, y_pred)\n",
    "# final_rmse = np.sqrt(final_mse)\n",
    "# final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# # Print final evaluation metrics on the test data\n",
    "# print(\"\\nFinal Evaluation on Test Set:\")\n",
    "# print(f\"MAE : {final_mae:.2f}\")\n",
    "# print(f\"MSE : {final_mse:.2f}\")\n",
    "# print(f\"RMSE: {final_rmse:.2f}\")\n",
    "# print(f\"R²  : {final_r2:.2f}\")\n",
    "\n",
    "# # --- Display Top Models from Search ---\n",
    "# results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# # Convert negative scores to positive\n",
    "# results_df['mean_test_MAE'] = -results_df['mean_test_MAE']\n",
    "# results_df['mean_test_MSE'] = -results_df['mean_test_MSE']\n",
    "\n",
    "# # Sort results by the best metric (MAE)\n",
    "# results_df = results_df.sort_values(by='rank_test_MAE')\n",
    "\n",
    "# # Display the top 5 models\n",
    "# print(\"\\n--- Top 5 Models by MAE ---\")\n",
    "# print(results_df[[\n",
    "#     'params', \n",
    "#     'mean_test_MAE', \n",
    "#     'mean_test_MSE',\n",
    "#     'mean_test_R2'\n",
    "# ]].head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv - project 3)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
